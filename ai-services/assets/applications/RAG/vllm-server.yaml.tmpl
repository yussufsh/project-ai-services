apiVersion: v1
kind: Pod
metadata:
  name: "{{ .AppName }}--vllm-server"
  labels:
    ai-services.io/application: "{{ .AppName }}"
  annotations:
    ai-services.io/model1: BAAI/bge-reranker-v2-m3
    ai-services.io/model2: ibm-granite/granite-embedding-278m-multilingual
    ai-services.io/model3: ibm-granite/granite-3.3-8b-instruct
    ai-services.io/instruct--sypre-cards: "4"
spec:
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
  containers:
    - name: instruct
      image: icr.io/ibmaiu_internal/ppc64le/dd2/spyre-vllm:v1.0.1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 600
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      env:
        - name: VLLM_MODEL_PATH
          value: "ibm-granite/granite-3.3-8b-instruct"
        - name: TORCH_SENDNN_CACHE_ENABLE
          value: "1"
        - name: VLLM_SPYRE_REQUIRE_PRECOMPILED_DECODERS
          value: "1"
        - name: VLLM_SPYRE_USE_CB
          value: "1"
        - name: MAX_MODEL_LEN
          value: "32768"
        - name: MAX_BATCH_SIZE
          value: "32"
        {{- range $k, $v := index .env "instruct" }}
        - name: {{ $k }}
          value: "{{ $v }}"
        {{- end }}
      resources:
        requests:
          podman.io/device=/dev/vfio: 4
          memory: "200Gi"
        limits:
          memory: "200Gi"
      ports:
        - containerPort: 8000
    - name: embedding
      image: icr.io/ppc64le-oss/vllm-ppc64le:0.9.1
      command: ["/bin/sh", "-c"]
      args: [
          "vllm serve ibm-granite/granite-embedding-278m-multilingual --port 8001"
      ]
      livenessProbe:
        httpGet:
          path: /health
          port: 8001
        initialDelaySeconds: 600
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          memory: "10Gi"
        limits:
          memory: "10Gi"
      ports:
        - containerPort: 8001
    - name: reranker
      image: icr.io/ppc64le-oss/vllm-ppc64le:0.9.1
      command: ["/bin/sh", "-c"]
      args: [
          "vllm serve BAAI/bge-reranker-v2-m3 --port 8002"
      ]
      livenessProbe:
        httpGet:
          path: /health
          port: 8002
        initialDelaySeconds: 600
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          memory: "10Gi"
        limits:
          memory: "10Gi"
      ports:
        - containerPort: 8002
